{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ready to use pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Datasets\\\\processedAnimeReviews.csv')\n",
    "w2v_model = Word2Vec.load('Models\\\\w2vmodel.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "170072\n100\n"
    }
   ],
   "source": [
    "print(len(w2v_model.wv.vocab))\n",
    "print(w2v_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "character 363540\nanime 342406\nstory 224260\nshow 213578\nlike 207145\none 194067\nreally 159586\nepisode 146563\nseries 143603\ngood 131607\ntime 125444\nwell 115288\nfirst 109700\nget 107283\nmuch 102853\nmake 101059\neven 100143\nwould 93321\nalso 90920\nthing 87132\n"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def count_words(text: str):\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        vocab[token] += 1\n",
    "\n",
    "df['review'].apply(lambda x: count_words(x))\n",
    "for value, count in vocab.most_common(20):\n",
    "    print(value, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(81108, 300)\n(54073, 300)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_vectors = w2v_model.wv\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(len(word_vectors.vocab)))}\n",
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQ_LEN = 300\n",
    "\n",
    "reviews = list(df['review'].values)\n",
    "sentiment = list(df['sentiment'].values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews,sentiment,test_size=.4, random_state=42)\n",
    "\n",
    "X_train = [[word_index.get(word, 0) for word in review] for review in X_train]\n",
    "X_test = [[word_index.get(word, 0) for word in review] for review in X_test]\n",
    "\n",
    "# padding\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wv_dim = 100\n",
    "number_words = len(word_vectors.vocab)\n",
    "word_vector_matrix = (np.random.rand(number_words, wv_dim) - .5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        word_vector_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout,SpatialDropout1D, Bidirectional, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.backend import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_layer = Embedding(number_words, wv_dim, mask_zero=False, weights=[word_vector_matrix], input_length=MAX_SEQ_LEN, trainable=False)\n",
    "averaging_layer = Lambda(lambda x: mean(x, axis=1))\n",
    "\n",
    "# Inputs\n",
    "review_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "embedded_sequences = wv_layer(review_input)\n",
    "\n",
    "# biGRU\n",
    "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n",
    "\n",
    "# Output\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "preds = averaging_layer(preds)\n",
    "\n",
    "# build the model\n",
    "model = Model(inputs=[review_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.001, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(81108, 300)\n(81108,)\nTrain on 72997 samples, validate on 8111 samples\nEpoch 1/3\n72997/72997 [==============================] - 2851s 39ms/sample - loss: 0.5710 - acc: 0.7426 - val_loss: 0.4988 - val_acc: 0.8015\nEpoch 2/3\n72997/72997 [==============================] - 2989s 41ms/sample - loss: 0.5016 - acc: 0.8060 - val_loss: 0.4949 - val_acc: 0.8015\nEpoch 3/3\n72997/72997 [==============================] - 3004s 41ms/sample - loss: 0.4924 - acc: 0.8078 - val_loss: 0.5055 - val_acc: 0.8015\n"
    }
   ],
   "source": [
    "nd_y_train = np.asarray(y_train)\n",
    "print(X_train.shape)\n",
    "print(nd_y_train.shape) \n",
    "hist = model.fit(X_train, nd_y_train, validation_split=0.1, epochs=3, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 300)]             0         \n_________________________________________________________________\nembedding (Embedding)        (None, 300, 100)          17007200  \n_________________________________________________________________\nspatial_dropout1d (SpatialDr (None, 300, 100)          0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               84480     \n_________________________________________________________________\ndropout (Dropout)            (None, 128)               0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 128)               512       \n_________________________________________________________________\ndense (Dense)                (None, 1)                 129       \n_________________________________________________________________\nlambda (Lambda)              (None,)                   0         \n=================================================================\nTotal params: 17,092,321\nTrainable params: 84,865\nNon-trainable params: 17,007,456\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "yhat_probabilities = model.predict(X_test, verbose=0)\n",
    "\t\n",
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(y_test, yhat_classes)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(y_test, yhat_classes)\n",
    "# print(f'Precision: {precision}')\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(y_test, yhat_classes)\n",
    "# print(f'Recall: {recall}')"
   ]
  }
 ]
}