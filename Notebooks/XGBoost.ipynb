{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xGBoost random forest model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV,RandomizedSearchCV, train_test_split,StratifiedKFold\n",
    "import xgboost as gb\n",
    "\n",
    "import utility as util\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workName</th>\n",
       "      <th>overallRating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8121</td>\n",
       "      <td>Cowboy_Bebop</td>\n",
       "      <td>10</td>\n",
       "      <td>cowboy bebop episodic series episodic mean one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63480</td>\n",
       "      <td>Utawarerumono</td>\n",
       "      <td>8</td>\n",
       "      <td>utawarerumono manages one harem anime anyone p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8452</td>\n",
       "      <td>Hajime_no_Ippo</td>\n",
       "      <td>10</td>\n",
       "      <td>first let say fan boxing fact pretty much hate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66544</td>\n",
       "      <td>Gensoumaden_Saiyuuki</td>\n",
       "      <td>9</td>\n",
       "      <td>saiyuki one anime grab first episode let go ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55936</td>\n",
       "      <td>Ranma_½</td>\n",
       "      <td>7</td>\n",
       "      <td>comedy romance based manga rumiko takahashi ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22039</td>\n",
       "      <td>Kino_no_Tabi__The_Beautiful_World</td>\n",
       "      <td>9</td>\n",
       "      <td>say anime traveler journeying different countr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68626</td>\n",
       "      <td>Kareshi_Kanojo_no_Jijou</td>\n",
       "      <td>8</td>\n",
       "      <td>kare kano romance anime could become incredibl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18797</td>\n",
       "      <td>Hunter_x_Hunter</td>\n",
       "      <td>10</td>\n",
       "      <td>overall best anime actually seen anything else...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43899</td>\n",
       "      <td>Golden_Boy</td>\n",
       "      <td>10</td>\n",
       "      <td>overall honestly really care others opinion an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18796</td>\n",
       "      <td>Hunter_x_Hunter</td>\n",
       "      <td>10</td>\n",
       "      <td>think hear anime people killing poor cute anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                workName  overallRating  \\\n",
       "id                                                        \n",
       "8121                        Cowboy_Bebop             10   \n",
       "63480                      Utawarerumono              8   \n",
       "8452                      Hajime_no_Ippo             10   \n",
       "66544               Gensoumaden_Saiyuuki              9   \n",
       "55936                            Ranma_½              7   \n",
       "22039  Kino_no_Tabi__The_Beautiful_World              9   \n",
       "68626            Kareshi_Kanojo_no_Jijou              8   \n",
       "18797                    Hunter_x_Hunter             10   \n",
       "43899                         Golden_Boy             10   \n",
       "18796                    Hunter_x_Hunter             10   \n",
       "\n",
       "                                                  review  sentiment  \n",
       "id                                                                   \n",
       "8121   cowboy bebop episodic series episodic mean one...          1  \n",
       "63480  utawarerumono manages one harem anime anyone p...          1  \n",
       "8452   first let say fan boxing fact pretty much hate...          1  \n",
       "66544  saiyuki one anime grab first episode let go ev...          1  \n",
       "55936  comedy romance based manga rumiko takahashi ra...          1  \n",
       "22039  say anime traveler journeying different countr...          1  \n",
       "68626  kare kano romance anime could become incredibl...          1  \n",
       "18797  overall best anime actually seen anything else...          1  \n",
       "43899  overall honestly really care others opinion an...          1  \n",
       "18796  think hear anime people killing poor cute anim...          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"../Datasets/processedAnimeReviews.csv\",index_col = 'id')\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('../Models/w2vmodel.bin')\n",
    "# Get mean feature vector of all words in a sentence\n",
    "def meanFeatureVec(sentence, word_vectors):\n",
    "    word_vecs = [word_vectors[word] for word in word_tokenize(sentence)]\n",
    "    mean_vec = np.asarray(word_vecs).mean(axis=0)\n",
    "    return mean_vec\n",
    "\n",
    "# Takes a dataframe of the reviews and returns a new dataframe of the word embeddings per review\n",
    "def reviewToVectors(sentences, word_vectors):\n",
    "    sent_vecs = [meanFeatureVec(sentence, word_vectors) for sentence in sentences]\n",
    "    df = pd.DataFrame(sent_vecs, index=sentences.index)\n",
    "    return df\n",
    "# Convert reviews to word embeddings\n",
    "X_vectors = reviewToVectors(reviews['review'], w2v_model.wv)\n",
    "# Split into train and test\n",
    "y = reviews['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Baseline\n",
    "   All possible parameters for xgboost.XGBClassifier are:\n",
    "   ----------\n",
    "   max_depth : [0,infinity)\n",
    "   \n",
    "       Maximum tree depth for base learners.\n",
    "\n",
    "   learning_rate : float\n",
    "   \n",
    "       Boosting learning rate (xgb's \"eta\")\n",
    "\n",
    "   n_estimators : int\n",
    "   \n",
    "       Number of trees to fit.\n",
    "\n",
    "\n",
    "   objective : string or callable\n",
    "       Specify the learning task and the corresponding learning objective or\n",
    "       a custom objective function to be used. There are \n",
    "\n",
    "   gamma : float\n",
    "       Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "   min_child_weight : int\n",
    "       Minimum sum of instance weight(hessian) needed in a child.\n",
    "\n",
    "   max_delta_step : int\n",
    "       Maximum delta step we allow each tree's weight estimation to be.\n",
    "\n",
    "   subsample : float\n",
    "       Subsample ratio of the training instance.\n",
    "\n",
    "\n",
    "\n",
    "   reg_alpha : float (xgb's alpha)\n",
    "       L1 regularization term on weights\n",
    "\n",
    "   reg_lambda : float (xgb's lambda)\n",
    "       L2 regularization term on weights\n",
    "\n",
    "   scale_pos_weight : float\n",
    "       Balancing of positive and negative weights.\n",
    "\n",
    "   base_score:\n",
    "      The initial prediction score of all instances, global bias.\n",
    "\n",
    "Not all of these paramaters affect accuracy of the model. The relevant once are:\n",
    "\n",
    "Also it is important to note that we only have one X column so :\n",
    "\n",
    "   colsample_bytree : float\n",
    "       Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "   colsample_bylevel : float\n",
    "       Subsample ratio of columns for each level.\n",
    "\n",
    "   colsample_bynode : float\n",
    "       Subsample ratio of columns for each split.\n",
    "       \n",
    "must all be set to 1 which is there default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'recall', 'precision', 'f1', 'roc_auc']\n",
    "#RFModel = gb.XGBRFClassifier()\n",
    "#util.cross_validate_scores(RFModel, X_train, y_train, cv=5, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will test out different objective methods for the XGBClassifier. There are 12 different objective functions\n",
    "we can define. We will break down this report into 12 seperate testing grounds and tweak each parameter for them.\n",
    "The 12 are:\n",
    "\n",
    "reg:squarederror: regression with squared loss. This is the default value\n",
    "\n",
    "reg:logistic: logistic regression\n",
    "\n",
    "binary:logistic: logistic regression for binary classification\n",
    "\n",
    "binary:hinge: hinge loss for binary classification. \n",
    "\n",
    "count:poisson –poisson regression for count data\n",
    "\n",
    "survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR).\n",
    "\n",
    "multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n",
    "\n",
    "rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n",
    "\n",
    "rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n",
    "\n",
    "rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized\n",
    "\n",
    "reg:gamma: gamma regression with log-link. \n",
    "\n",
    "reg:tweedie: Tweedie regression with log-link. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will test the baseline for each objective. Then we will break down each objective and do furthur testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Squared Log Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.4min remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8473 (0.0002)\n",
      "recall: 0.9784 (0.0002)\n",
      "precision: 0.8541 (0.0002)\n",
      "f1: 0.9121 (0.0001)\n",
      "roc_auc: 0.8560 (0.0004)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8436 (0.0013)\n",
      "recall: 0.9772 (0.0009)\n",
      "precision: 0.8514 (0.0009)\n",
      "f1: 0.9100 (0.0008)\n",
      "roc_auc: 0.8429 (0.0010)\n",
      "Logistical regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.5min remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8509 (0.0002)\n",
      "recall: 0.9723 (0.0002)\n",
      "precision: 0.8613 (0.0002)\n",
      "f1: 0.9134 (0.0001)\n",
      "roc_auc: 0.8592 (0.0004)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8463 (0.0012)\n",
      "recall: 0.9700 (0.0014)\n",
      "precision: 0.8584 (0.0005)\n",
      "f1: 0.9108 (0.0007)\n",
      "roc_auc: 0.8468 (0.0014)\n",
      "Logistical regression for binary classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.6min remaining:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8509 (0.0002)\n",
      "recall: 0.9723 (0.0002)\n",
      "precision: 0.8613 (0.0002)\n",
      "f1: 0.9134 (0.0001)\n",
      "roc_auc: 0.8592 (0.0004)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8463 (0.0012)\n",
      "recall: 0.9700 (0.0014)\n",
      "precision: 0.8584 (0.0005)\n",
      "f1: 0.9108 (0.0007)\n",
      "roc_auc: 0.8468 (0.0014)\n",
      "Binary classification using hinge loss optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.5min remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8245 (0.0007)\n",
      "recall: 0.9977 (0.0001)\n",
      "precision: 0.8230 (0.0006)\n",
      "f1: 0.9020 (0.0003)\n",
      "roc_auc: 0.5443 (0.0019)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8229 (0.0014)\n",
      "recall: 0.9972 (0.0006)\n",
      "precision: 0.8219 (0.0013)\n",
      "f1: 0.9011 (0.0007)\n",
      "roc_auc: 0.5409 (0.0039)\n",
      "Poisson regression for count data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.5min remaining:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8457 (0.0003)\n",
      "recall: 0.9789 (0.0002)\n",
      "precision: 0.8523 (0.0003)\n",
      "f1: 0.9112 (0.0001)\n",
      "roc_auc: 0.8521 (0.0002)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8425 (0.0012)\n",
      "recall: 0.9776 (0.0011)\n",
      "precision: 0.8502 (0.0008)\n",
      "f1: 0.9094 (0.0007)\n",
      "roc_auc: 0.8391 (0.0017)\n",
      "Survival Cox Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.3min remaining:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8090 (0.0000)\n",
      "recall: 1.0000 (0.0000)\n",
      "precision: 0.8090 (0.0000)\n",
      "f1: 0.8944 (0.0000)\n",
      "roc_auc: 0.5111 (0.0107)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8090 (0.0000)\n",
      "recall: 1.0000 (0.0000)\n",
      "precision: 0.8090 (0.0000)\n",
      "f1: 0.8944 (0.0000)\n",
      "roc_auc: 0.5164 (0.0140)\n",
      "Classification by optimizing softmax objective\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  5.0min remaining:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8509 (0.0002)\n",
      "recall: 0.9722 (0.0001)\n",
      "precision: 0.8613 (0.0002)\n",
      "f1: 0.9134 (0.0001)\n",
      "roc_auc: 0.6545 (0.0007)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8462 (0.0014)\n",
      "recall: 0.9698 (0.0013)\n",
      "precision: 0.8584 (0.0007)\n",
      "f1: 0.9107 (0.0008)\n",
      "roc_auc: 0.6462 (0.0020)\n",
      "Rank Pairwise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.6min remaining:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.7469 (0.0007)\n",
      "recall: 0.7395 (0.0010)\n",
      "precision: 0.9339 (0.0002)\n",
      "f1: 0.8254 (0.0006)\n",
      "roc_auc: 0.8437 (0.0003)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.7409 (0.0014)\n",
      "recall: 0.7355 (0.0015)\n",
      "precision: 0.9295 (0.0009)\n",
      "f1: 0.8212 (0.0010)\n",
      "roc_auc: 0.8343 (0.0012)\n",
      "Rank ndcg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   48.6s remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   50.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.1910 (0.0000)\n",
      "recall: 0.0000 (0.0000)\n",
      "precision: 0.0000 (0.0000)\n",
      "f1: 0.0000 (0.0000)\n",
      "roc_auc: 0.5000 (0.0000)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.1910 (0.0000)\n",
      "recall: 0.0000 (0.0000)\n",
      "precision: 0.0000 (0.0000)\n",
      "f1: 0.0000 (0.0000)\n",
      "roc_auc: 0.5000 (0.0000)\n",
      "Rank map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   44.5s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   45.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.1910 (0.0000)\n",
      "recall: 0.0000 (0.0000)\n",
      "precision: 0.0000 (0.0000)\n",
      "f1: 0.0000 (0.0000)\n",
      "roc_auc: 0.5000 (0.0000)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.1910 (0.0000)\n",
      "recall: 0.0000 (0.0000)\n",
      "precision: 0.0000 (0.0000)\n",
      "f1: 0.0000 (0.0000)\n",
      "roc_auc: 0.5000 (0.0000)\n",
      "Regression gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.5min remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8498 (0.0004)\n",
      "recall: 0.9658 (0.0007)\n",
      "precision: 0.8644 (0.0003)\n",
      "f1: 0.9123 (0.0003)\n",
      "roc_auc: 0.8482 (0.0002)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8442 (0.0010)\n",
      "recall: 0.9626 (0.0015)\n",
      "precision: 0.8612 (0.0007)\n",
      "f1: 0.9091 (0.0006)\n",
      "roc_auc: 0.8370 (0.0020)\n",
      "Regression tweedie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.5min remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores\n",
      "accuracy: 0.8479 (0.0002)\n",
      "recall: 0.9730 (0.0004)\n",
      "precision: 0.8580 (0.0002)\n",
      "f1: 0.9119 (0.0001)\n",
      "roc_auc: 0.8505 (0.0004)\n",
      "\n",
      "Validation Scores\n",
      "accuracy: 0.8438 (0.0012)\n",
      "recall: 0.9711 (0.0014)\n",
      "precision: 0.8554 (0.0006)\n",
      "f1: 0.9096 (0.0007)\n",
      "roc_auc: 0.8386 (0.0014)\n"
     ]
    }
   ],
   "source": [
    "SquaredError = gb.XGBClassifier(objective = \"reg:squarederror\") \n",
    "Logistic = gb.XGBClassifier(objective = \"reg:logistic\") \n",
    "BinaryLogistic = gb.XGBClassifier(objective = \"binary:logistic\") \n",
    "BinaryHinge = gb.XGBClassifier(objective = \"binary:hinge\") \n",
    "CountPoisson = gb.XGBClassifier(objective = \"count:poisson\") \n",
    "SurvivalCox = gb.XGBClassifier(objective = \"survival:cox\") \n",
    "MultiSoftMax = gb.XGBClassifier(objective = \"multi:softmax\", num_class = 2) \n",
    "RankPairWise = gb.XGBClassifier(objective = \"rank:pairwise\") \n",
    "RankNDCG = gb.XGBClassifier(objective = \"rank:ndcg\") \n",
    "RankMap = gb.XGBClassifier(objective = \"rank:map\") \n",
    "RegGamma = gb.XGBClassifier(objective = \"reg:gamma\") \n",
    "RegTweedie = gb.XGBClassifier(objective = \"reg:tweedie\") \n",
    "\n",
    "print(\"Regression Squared Error\")\n",
    "util.cross_validate_scores(SquaredError, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Logistical regression\")\n",
    "util.cross_validate_scores(Logistic, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Logistical regression for binary classification\")\n",
    "util.cross_validate_scores(BinaryLogistic, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Binary classification using hinge loss optimization\")\n",
    "util.cross_validate_scores(BinaryHinge, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Poisson regression for count data\")\n",
    "util.cross_validate_scores(CountPoisson, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Survival Cox Regression\")\n",
    "util.cross_validate_scores(SurvivalCox, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Classification by optimizing softmax objective\")\n",
    "util.cross_validate_scores(MultiSoftMax, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Rank Pairwise\")\n",
    "util.cross_validate_scores(RankPairWise, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Rank ndcg\")\n",
    "util.cross_validate_scores(RankNDCG, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Rank map\")\n",
    "util.cross_validate_scores(RankMap, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Regression gamma\")\n",
    "util.cross_validate_scores(RegGamma, X_train, y_train, cv=5, metrics=metrics)\n",
    "\n",
    "print(\"Regression tweedie\")\n",
    "util.cross_validate_scores(RegTweedie, X_train, y_train, cv=5, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AbsoluteBaseline = gb.XGBClassifier() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error Objective 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  30 out of  30 | elapsed: 25.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           learning_rate=0.1, max_delta_step=0,\n",
       "                                           max_depth=3, min_child_weight=1,\n",
       "                                           missing=None, n_estimators=100,\n",
       "                                           n_jobs=1, nthread=N...\n",
       "                                                          0.8, 0.9],\n",
       "                                        'max_delta_step': [0],\n",
       "                                        'max_depth': [1, 3, 5, 7, 9, 11],\n",
       "                                        'min_child_weight': [0, 5, 10, 15],\n",
       "                                        'n_estimators': [100],\n",
       "                                        'objective': ['reg:squarederror'],\n",
       "                                        'reg_alpha': [0, 0.001, 0.005, 0.01,\n",
       "                                                      0.05],\n",
       "                                        'reg_lambda': [0, 0.1, 0.3, 0.6, 9],\n",
       "                                        'scale_pos_weight': [0, 0.1, 0.3, 0.6,\n",
       "                                                             9]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scale_pos_weight': 0.3, 'reg_lambda': 0.6, 'reg_alpha': 0.001, 'objective': 'reg:squarederror', 'n_estimators': 100, 'min_child_weight': 15, 'max_depth': 3, 'max_delta_step': 0, 'learning_rate': 0.1, 'gamma': 15}\n"
     ]
    }
   ],
   "source": [
    "parametersMSE = {'objective':['reg:squarederror'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfMSE = RandomizedSearchCV(AbsoluteBaseline, parametersMSE, n_jobs=5, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfMSE.fit( X_train,y_train)\n",
    "\n",
    "print(clfMSE.cv_results_['params'][clfMSE.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical regression for binary classification Objective 3\n",
    "This is the exact same for objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 15.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bynode=1,\n",
       "                                           colsample_bytree=1, gamma=0,\n",
       "                                           learning_rate=0.1, max_delta_step=0,\n",
       "                                           max_depth=3, min_child_weight=1,\n",
       "                                           missing=None, n_estimators=100,\n",
       "                                           n_jobs=1, nthread=N...\n",
       "                                                          0.8, 0.9],\n",
       "                                        'max_delta_step': [0],\n",
       "                                        'max_depth': [1, 3, 5, 7, 9, 11],\n",
       "                                        'min_child_weight': [0, 5, 10, 15],\n",
       "                                        'n_estimators': [100],\n",
       "                                        'objective': ['binary:logistic'],\n",
       "                                        'reg_alpha': [0, 0.001, 0.005, 0.01,\n",
       "                                                      0.05],\n",
       "                                        'reg_lambda': [0, 0.1, 0.3, 0.6, 9],\n",
       "                                        'scale_pos_weight': [0, 0.1, 0.3, 0.6,\n",
       "                                                             9]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scale_pos_weight': 0.3, 'reg_lambda': 0, 'reg_alpha': 0.005, 'objective': 'binary:logistic', 'n_estimators': 100, 'min_child_weight': 15, 'max_depth': 3, 'max_delta_step': 0, 'learning_rate': 0.2, 'gamma': 5}\n"
     ]
    }
   ],
   "source": [
    "parametersLRB = {'objective':['binary:logistic'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfLRB = RandomizedSearchCV(AbsoluteBaseline, parametersLRB, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfLRB.fit( X_train,y_train)\n",
    "\n",
    "print(clfLRB.cv_results_['params'][clfLRB.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinge loss for binary classificative Objective 4\n",
    "\n",
    "$\\ell(y) = \\max(0, 1-t \\cdot y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersHL = {'objective':['binary:hinge'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfHL = RandomizedSearchCV(AbsoluteBaseline, parametersHL, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfHL.fit( X_train,y_train)\n",
    "\n",
    "print(clfHL.cv_results_['params'][clfHL.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson regression for count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersPRC = {'objective':['count:poisson'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfPRC = RandomizedSearchCV(AbsoluteBaseline, parametersPRC, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfPRC.fit( X_train,y_train)\n",
    "\n",
    "print(clfPRC.cv_results_['params'][clfPRC.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersSC = {'objective':['survival:cox'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfSC = RandomizedSearchCV(AbsoluteBaseline, parametersPRC, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfSC.fit( X_train,y_train)\n",
    "\n",
    "print(clfSC.cv_results_['params'][clfSC.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi:softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersMSM = {'objective':['multi:softmax'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'num_class':[2]}\n",
    "clfMSM = RandomizedSearchCV(AbsoluteBaseline, parametersMSM, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfMSM.fit( X_train,y_train)\n",
    "\n",
    "print(clfMSM.cv_results_['params'][clfMSM.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rank pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parametersRPW = {'objective':['rank:pairwise'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfRPW = RandomizedSearchCV(AbsoluteBaseline, parametersRPW, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfRPW.fit( X_train,y_train)\n",
    "\n",
    "print(clfRPW.cv_results_['params'][clfRPW.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rank ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parametersNDCG = {'objective':['rank:ndcg'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfNDCG = RandomizedSearchCV(AbsoluteBaseline, parametersNDCG, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfNDCG.fit( X_train,y_train)\n",
    "\n",
    "print(clfNDCG.cv_results_['params'][clfNDCG.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rank map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersMAP = {'objective':['rank:map'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfMAP = RandomizedSearchCV(AbsoluteBaseline, parametersMAP, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfMAP.fit( X_train,y_train)\n",
    "\n",
    "print(clfMAP.cv_results_['params'][clfMAP.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reg gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersGam = {'objective':['reg:gamma'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfGam = RandomizedSearchCV(AbsoluteBaseline, parametersGam, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfGam.fit( X_train,y_train)\n",
    "\n",
    "print(clfGam.cv_results_['params'][clfGam.best_index_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reg tweedie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersTwd = {'objective':['reg:tweedie'],\n",
    "              'learning_rate': [x * 0.1 for x in range(0, 10)], #so called `eta` value\n",
    "              'max_depth': [i for i in range(1,12,2)],\n",
    "              'min_child_weight': [i for i in range(0,20,5)],\n",
    "              'n_estimators': [100],\n",
    "              'gamma' : [i for i in range(0,20,5)],\n",
    "              'max_delta_step':[0],\n",
    "              'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "              'reg_lambda':[0, 0.1, 0.3, 0.6, 9],\n",
    "              'scale_pos_weight':[0, 0.1, 0.3, 0.6, 9]\n",
    "             }\n",
    "clfTwd = RandomizedSearchCV(AbsoluteBaseline, parametersTwd, n_jobs=-1, \n",
    "                   cv=StratifiedKFold(), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clfTwd.fit( X_train,y_train)\n",
    "\n",
    "print(clfTwd.cv_results_['params'][clfTwd.best_index_])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
